{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from data import Dataset\n",
    "from model import Encoder, GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = 'nasbench201'\n",
    "dataset = Dataset(space=space, root='~/datasets')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(dataset.available_ops), 300, 256, torch.nn.PReLU(), k=2, skip=False)\n",
    "model = GRACE(encoder, 256, 32, 0.4)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(len(batch))\n",
    "    x, g = model(batch.x, batch.edge_index, batch.ptr)\n",
    "    print(x.size())\n",
    "    print(g.size())\n",
    "    loss, node_loss, graph_loss = model.loss(x, x, g, g, batch.ptr)\n",
    "    print(loss, node_loss, graph_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "\n",
    "sklearn_pca = PCA(n_components=2)\n",
    "data_2d = sklearn_pca.fit_transform(X)\n",
    "plt.figure()\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图嵌入可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from itertools import chain\n",
    "from torch_geometric.data import InMemoryDataset, Data, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nasbench101\n",
    "root = '/home/jingkun/datasets'\n",
    "from nasbench import api as NASBench101API\n",
    "data_path = os.path.join(root, 'nasbench_full.tfrecord')\n",
    "nasbench = NASBench101API.NASBench(data_path)\n",
    "available_ops = ['input', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nasbench201\n",
    "root = '/home/jingkun/datasets'\n",
    "from nas_201_api import NASBench201API\n",
    "data_path = os.path.join(root, 'NAS-Bench-201-v1_1-096897.pth')\n",
    "nasbench = NASBench201API(data_path, verbose=False)\n",
    "available_ops = ['input', 'none', 'skip_connect', 'nor_conv_1x1', 'nor_conv_3x3', 'avg_pool_3x3', 'output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nasbench201_to_nasbench101(arch_list):\n",
    "    num_ops = sum(range(1, 1 + len(arch_list))) + 2\n",
    "    adj = np.zeros((num_ops, num_ops), dtype=np.uint8)\n",
    "    ops = ['input', 'output']\n",
    "    node_lists = [[0]]\n",
    "    for node_201 in arch_list:\n",
    "        node_list = []\n",
    "        for node in node_201:\n",
    "            node_idx = len(ops) - 1\n",
    "            adj[node_lists[node[1]], node_idx] = 1\n",
    "            ops.insert(-1, node[0])\n",
    "            node_list.append(node_idx)\n",
    "        node_lists.append(node_list)\n",
    "    adj[-(1+len(arch_list)):-1, -1] = 1\n",
    "    arch = {'adj': adj,\n",
    "            'ops': ops,}\n",
    "    return arch\n",
    "\n",
    "\n",
    "def arch2Data(arch):\n",
    "    x = torch.tensor([available_ops.index(x) for x in arch['ops']], dtype=torch.long)\n",
    "    if 'acc' in arch.keys():\n",
    "        y = torch.ones_like(x) * arch['acc']\n",
    "    else:\n",
    "        y = None\n",
    "    forward_edges = [[(i, j) for j, x in enumerate(xs) if x > 0] for i, xs in enumerate(arch['adj'])]\n",
    "    forward_edges = np.array(list(chain(*forward_edges)))\n",
    "    backward_edges = forward_edges[::-1, ::-1]\n",
    "    edges = np.concatenate([forward_edges, backward_edges])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, y=y, edge_index=edge_index)\n",
    "    return data\n",
    "\n",
    "# def arch2Data(arch):\n",
    "#     x = torch.tensor([available_ops.index(x) for x in arch['ops']], dtype=torch.long)\n",
    "#     if 'acc' in arch.keys():\n",
    "#         y = torch.ones_like(x) * arch['acc']\n",
    "#     else:\n",
    "#         y = None\n",
    "#     forward_edges = [[(i, j) for j, x in enumerate(xs) if x > 0] for i, xs in enumerate(arch['adj'])]\n",
    "#     forward_edges = np.array(list(chain(*forward_edges)))\n",
    "#     backward_edges = forward_edges[::-1, ::-1]\n",
    "#     edges = np.concatenate([forward_edges, backward_edges])\n",
    "#     edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "#     data = Data(x=x, y=y, edge_index=edge_index)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch1 = {'adj': np.array([[0, 1, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv1x1-bn-relu', 'maxpool3x3', 'conv1x1-bn-relu', 'maxpool3x3', 'conv3x3-bn-relu', 'output'],}\n",
    "arch2 = {'adj': np.array([[0, 1, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv3x3-bn-relu', 'maxpool3x3', 'conv1x1-bn-relu', 'maxpool3x3', 'conv3x3-bn-relu', 'output'],}\n",
    "arch3 = {'adj': np.array([[0, 1, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 1, 1, 0, 1, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv1x1-bn-relu', 'conv1x1-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'output'],}\n",
    "arch4 = {'adj': np.array([[0, 1, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 1, 1, 0, 1, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'output'],}\n",
    "arch5 = {'adj': np.array([[0, 1, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv1x1-bn-relu', 'conv1x1-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'conv3x3-bn-relu', 'output'],}\n",
    "arch6 = {'adj': np.array([[0, 1, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv1x1-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'conv3x3-bn-relu', 'output'],}\n",
    "arch7 = {'adj': np.array([[0, 1, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv1x1-bn-relu', 'conv1x1-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'output'],}\n",
    "arch8 = {'adj': np.array([[0, 1, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'conv1x1-bn-relu', 'conv3x3-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'output'],}\n",
    "arch9 = {'adj': np.array([[0, 1, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0, 0, 0]]),\n",
    "         'ops': ['input', 'maxpool3x3', 'conv1x1-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'conv3x3-bn-relu', 'output'],}\n",
    "fixed = nasbench.fixed_statistics[list(nasbench.hash_iterator())[100000]]\n",
    "arch10 = {'adj': fixed['module_adjacency'], 'ops': fixed['module_operations']}\n",
    "archs = [(arch1,), (arch2,), (arch3,), (arch4,), (arch5,), (arch6,), (arch7,), (arch8,), (arch9,), (arch10,)]\n",
    "\n",
    "print(archs)\n",
    "data_list = []\n",
    "for arch in archs:\n",
    "    data_list.append(arch2Data(arch[0]))\n",
    "batch = Batch.from_data_list(data_list)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_1  = [(('avg_pool_3x3', 0),), (('nor_conv_1x1', 0), ('skip_connect', 1)), (('nor_conv_1x1', 0), ('skip_connect', 1), ('skip_connect', 2))]\n",
    "arch_1_ = [(('avg_pool_3x3', 0),), (('nor_conv_1x1', 0), ('nor_conv_3x3', 1)), (('nor_conv_1x1', 0), ('skip_connect', 1), ('skip_connect', 2))]\n",
    "arch_2  = [(('nor_conv_1x1', 0),), (('nor_conv_3x3', 0), ('skip_connect', 1)), (('skip_connect', 0), ('skip_connect', 1), ('avg_pool_3x3', 2))]\n",
    "arch_2_ = [(('nor_conv_1x1', 0),), (('nor_conv_3x3', 0), ('nor_conv_3x3', 1)), (('skip_connect', 0), ('skip_connect', 1), ('avg_pool_3x3', 2))]\n",
    "arch_3  = [(('nor_conv_3x3', 0),), (('nor_conv_3x3', 0), ('skip_connect', 1)), (('avg_pool_3x3', 0), ('skip_connect', 1), ('avg_pool_3x3', 2))]\n",
    "arch_3_ = [(('nor_conv_3x3', 0),), (('nor_conv_3x3', 0), ('nor_conv_3x3', 1)), (('avg_pool_3x3', 0), ('skip_connect', 1), ('avg_pool_3x3', 2))]\n",
    "archs = [(arch_1,), (arch_1_,), (arch_2,), (arch_2_,), (arch_3,), (arch_3_,), ]\n",
    "\n",
    "arch_1  = nasbench201_to_nasbench101(arch_1)\n",
    "arch_1_ = nasbench201_to_nasbench101(arch_1_)\n",
    "arch_2  = nasbench201_to_nasbench101(arch_2)\n",
    "arch_2_ = nasbench201_to_nasbench101(arch_2_)\n",
    "\n",
    "print(archs)\n",
    "data_list = []\n",
    "for arch in archs:\n",
    "    a = nasbench201_to_nasbench101(arch[0])\n",
    "    data_list.append(arch2Data(a))\n",
    "batch = Batch.from_data_list(data_list)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_base_model, get_activation\n",
    "from model import PairWiseLearning_BARLOW_TWINS, PairWiseLearning_MVGRL, Predictor, MAELearning\n",
    "#encoder = Encoder(len(available_ops), 32, 128, get_activation('prelu'), base_model=get_base_model('GATConv'), k=3, skip=True, use_bn=True)\n",
    "encoder = MAELearning(len(available_ops), 16, 32, get_activation('prelu'), get_base_model('GATConv'), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '64')\n",
    "#encoder.load_state_dict(torch.load(os.path.join('finetune101_rep', 'finetune_encoder_449.pt')), False)\n",
    "#encoder.load_state_dict(torch.load(os.path.join('pretrain101_aug_mask_1', 'model_449.pt')))\n",
    "#encoder.load_state_dict(torch.load(os.path.join('pretrain101_aug_diff_1', 'model_449.pt')))\n",
    "#encoder.load_state_dict(torch.load(os.path.join('pretrain101_aug_mask_diff_1', 'model_449.pt')))\n",
    "#encoder.load_state_dict(torch.load(os.path.join('finetune101_rep_mask_1', 'finetune_encoder_449.pt')), False)\n",
    "#encoder.load_state_dict(torch.load(os.path.join('finetune101_rep_diff_1', 'finetune_encoder_449.pt')), False)\n",
    "#encoder.load_state_dict(torch.load(os.path.join('finetune101_rep_mask_diff_1', 'finetune_encoder_449.pt')), False)\n",
    "encoder.load_state_dict(torch.load(os.path.join('pretrain101', 'best_model_15712.pt')))\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "x          = batch.x\n",
    "edge_index = batch.edge_index\n",
    "ptr        = batch.ptr\n",
    "with torch.no_grad():\n",
    "    z, g = encoder(x, edge_index, ptr, x, edge_index, ptr)\n",
    "g = g.detach().numpy()\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "sklearn_pca = PCA(n_components=2)\n",
    "data_2d = sklearn_pca.fit_transform(g)\n",
    "print(data_2d)\n",
    "plt.figure()\n",
    "colors = ['red','green','blue','orange','pink', 'black']\n",
    "for i in range(data_2d.shape[0]//2-1):\n",
    "    plt.scatter(data_2d[2*i:2*(i+1), 0], data_2d[2*i:2*(i+1), 1], c=colors[i])\n",
    "plt.scatter(data_2d[8:9, 0], data_2d[8:9, 1], c=colors[-1])\n",
    "plt.scatter(data_2d[9:10, 0], data_2d[9:10, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
